# Module 5 Hadoop eco-system

Hadoop Ecosystem is neither a programming language nor a service, it is a platform or
framework which solves big data problems. You can consider it as a suite which
encompasses a number of services (ingesting, storing, analyzing and maintaining) inside it. 

Hadoop has a vast collection of tools, so we’ve divided them according to their roles in the Hadoop ecosystem.

## Storage of Data

### Zookeeper

Zookeeper helps you manage the naming conventions, configuration, synchronization, and other pieces of information of the Hadoop clusters. It is the open-source centralized server of the ecosystem. 

## Execution Engine

### Spark
see [module 4](module_4.md)

### MapReduce
see [module 3](module_3.md)

## Database Management

### Hive

HIVE is a data warehousing component which performs reading, writing
and managing large data sets in a distributed environment using SQL-like interface.

The query language of Hive is called Hive Query Language(HQL), which is very
similar like SQL

It has 2 basic components: Hive Command Line and JDBC/ODBC driver.
* The Hive Command line interface is used to execute HQL commands.
* While, Java Database Connectivity (JDBC) and Object Database Connectivity
(ODBC) is used to establish connection from data storage.

### Impla

Apache Impala is a query engine that runs on Apache Hadoop.

Impala is promoted for analysts and data scientists to perform analytics on data stored in Hadoop via SQL or business intelligence tools.

Impala can join itself with Hive’s meta store and share the required information with it.


## Abstraction

### Pig

PIG has two parts: Pig Latin, the language and the pig runtime, for the execution
environment. You can better understand it as Java and JVM.
It supports pig latin language, which has SQL like command structure.

How Pig works ?

In PIG, first the load command, loads the data. Then we perform various functions on it like
grouping, filtering, joining, sorting, etc. At last, either you can dump the data on the screen or
you can store the result back in HDFS

### Oozie

Consider Apache Oozie as a clock and alarm service inside Hadoop Ecosystem. For Apache
jobs, Oozie has been just like a scheduler. It schedules Hadoop jobs and binds them together
as one logical work.

There are two kinds of Oozie jobs:
1. Oozie workflow: These are sequential set of actions to be executed. You can assume
it as a relay race. Where each athlete waits for the last one to complete his part.
2. Oozie Coordinator: These are the Oozie jobs which are triggered when the data is
made available to it. Think of this as the response-stimuli system in our body. In the
same manner as we respond to an external stimulus, an Oozie coordinator responds to
the availability of data and it rests otherwise.


### Sqoop
The major difference between Flume and Sqoop is that:
* Flume only ingests unstructured data or semi-structured data into HDFS

* While Sqoop can import as well as export structured data from RDBMS or Enterprise
data warehouses to HDFS or vice versa

## Data Streaming

### Flume
Flume is a service which helps in ingesting unstructured and semi-structured data
into HDFS

It gives us a solution which is reliable and distributed and helps us in collecting,
aggregating and moving large amount of data sets.

It helps us to ingest online streaming data from various sources like network traffic,
social media, email messages, log files etc. in HDFS


### Kafka

Apache Kafka is a distributed data store optimized for ingesting and processing streaming data in real-time. Streaming data is data that is continuously generated by thousands of data sources, which typically send the data records in simultaneously. A streaming platform needs to handle this constant influx of data, and process the data sequentially and incrementally.

Kafka provides three main functions to its users:

* Publish and subscribe to streams of records
* Effectively store streams of records in the order in which records were generated
* Process streams of records in real time

## Machine Learning

### Mahout
Mahout provides an environment for creating machine learning applications which are scalable

It performs collaborative filtering, clustering and classification. Some people also
consider frequent item set missing as Mahout’s function. Let us understand them
individually

1. Collaborative filtering: Mahout mines user behaviors, their patterns and their
characteristics and based on that it predicts and make recommendations to the users.
The typical use case is E-commerce website.
2. Clustering: It organizes a similar group of data together like articles can contain
blogs, news, research papers etc.
3. Classification: It means classifying and categorizing data into various subdepartments like articles can be categorized into blogs, news, essay, research papers
and other categories.
4. Frequent item set missing: Here Mahout checks, which objects are likely to be
appearing together and make suggestions, if they are missing. For example, cell phone
and cover are brought together in general. So, if you search for a cell phone, it will
also recommend you the cover and cases


### Spark MLlib
Spark MLlib is used to perform machine learning in Apache Spark. MLlib consists of popular algorithms and utilities. MLlib in Spark is a scalable Machine learning library that focus on both high-quality algorithm and high speed. 

## Monitoring

### Ambari

Ambari is an Apache Software Foundation Project which aims at making Hadoop ecosystem
more manageable

It includes software for provisioning, managing and monitoring Apache Hadoop clusters

The Ambari provides:
1. Hadoop cluster provisioning:
* It gives us step by step process for installing Hadoop services across a number
of hosts.
* It also handles configuration of Hadoop services over a cluster.
2. Hadoop cluster management
* It provides a central management service for starting, stopping and reconfiguring Hadoop services across the cluster. 
3. Hadoop cluster monitoring
* For monitoring health and status, Ambari provides us a dashboard.
* The Amber Alert framework is an alerting service which notifies the user,
whenever the attention is needed. For example, if a node goes down or low
disk space on a node, etc.